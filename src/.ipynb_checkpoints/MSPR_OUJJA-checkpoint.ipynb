{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f35dc2-ac76-446b-8fcb-f1aa83304ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install minio\n",
    "!pip install requests\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f52b1903-38ca-494c-bcb6-ca0c61521db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.5.0)\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py4j\n",
      "Successfully installed py4j-0.10.9.7\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7579f39c-7b1d-4b42-9aad-fc38799ac999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import calendar\n",
    "from minio import Minio\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from grab_data import grab_data\n",
    "from dump_to_minio import dump_to_minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a64d7c-3a2c-41f5-ba0f-af1a8c15e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set critical environment variables\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.driver.extraJavaOptions=\"-Djava.security.manager=allow\" pyspark-shell'\n",
    "os.environ['SPARK_LOCAL_IP'] = '127.0.0.1'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"GrabAndDumpJob\") \\\n",
    ".master(\"spark://spark-master:7077\") \\\n",
    ".config(\"spark.submit.deployMode\", \"client\") \\\n",
    ".config(\"spark.driver.host\", \"spark-driver\") \\\n",
    ".config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    ".config(\"spark.executor.memory\", \"2g\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.access.key\", \"minio\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.secret.key\", \"minio123\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    ".config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    grab_data(2019, 10)\n",
    "    time_1 = time.time() - start_time\n",
    "    \n",
    "    dump_to_minio()\n",
    "    time_2 = time.time() - start_time\n",
    "\n",
    "    print(f\"Data processing time: {time_1:.2f} seconds\")\n",
    "    print(f\"Total execution time: {time_2:.2f} seconds\")\n",
    "finally:\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
